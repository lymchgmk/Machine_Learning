{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 5. 서포트 벡터 머신.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hLzK9F51HYS"
      },
      "source": [
        "## 5.5 연습문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umrpGA9p1HQK"
      },
      "source": [
        "### 1. 서포트 벡터 머신의 근본 아이디어는 무엇인가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgYBNZ7_13Nf"
      },
      "source": [
        "- A. 서포트 벡터 머신(SVM)의 근본적인 아이디어는 클래스 사이에 가능한 한 가장 넓은 '도로'를 내는 것입니다. 다시 말해 두 클래스를 구분하는 결정 경계와 샘플 사이의 마진을 가능한 한 가장 크게 하는 것이 목적입니다. 소프트 마진 분류를 수행할 때는 SVM이 두 클래스를 완벽하게 나누는 것과 가장 넓은 도로를 만드는 것 사이에 절충안을 찾습니다(즉, 몇 개의 샘플은 도로 안에 놓일 수 있습니다). 또 하나의 핵심적인 아이디어는 비선형 데이터셋에서 훈련할 때 커널 함수를 사용하는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdGeThQK1jXa"
      },
      "source": [
        "### 2. 서포트 벡터가 무엇인가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxLIV9Kw2Yy3"
      },
      "source": [
        "- A. **서포트 벡터**는 SVM이 훈련된 후에 경계를 포함해 도로에 놓인 어떤 샘플입니다(이전 해답을 참조하세요). 결정 경계는 전적으로 서포트 벡터에 의해 결정됩니다. 서포트 벡터가 아닌(즉, 도로 밖에 있는) 어떤 샘플도 영향을 주지 못합니다. 이런 샘플은 삭제하고 다른 샘플을 더 추가하거나, 다른 곳으로 이동시킬 수 있습니다. 샘플이 도로 밖에 있는 한 결정 경계에 영향을 주지 못할 것입니다. 예측을 계산할 때는 전체 훈련 세트가 아니라 서포트 벡터만 관여됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9MaCCTX1jMR"
      },
      "source": [
        "### 3. SVM을 사용할 때 입력값의 스케일이 왜 중요한가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX6ZDMNI2yzf"
      },
      "source": [
        "- A. SVM은 클래스 사이에 가능한 한 가장 큰 도로를 내는 것이므로(1번 답을 참조하세요) 훈련 세트의 스케일이 맞지 않으면 크기가 작은 특성을 무시하는 경향이 있습니다([그림 5-2] 참조)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8i6vxwz1jBZ"
      },
      "source": [
        "### 4. SVM 분류기가 샘플을 분류할 때 신뢰도 점수와 확률을 출력할 수 있나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez1L-Kh83Isq"
      },
      "source": [
        "- A. SVM 분류기는 테스트 샘플과 결정 경계 사이의 거리를 출력할 수 있으므로 이를 신뢰도 점수로 사용할 수 있습니다. 그러나 이 점수를 클래스 확률의 추정값으로 바로 변환할 수는 없습니다. 사이킷런에서 SVM 모델을 만들 때 `probability=True`로 설정하면 훈련이 끝난 후 (훈련 데이터에 5-겹 교차 검증을 사용하여 추가로 훈련시킨) SVM의 점수에 로지스틱 회귀를 훈련시켜 확률을 계산합니다. 이 설정은 SVM 모델에 `predict_proba()`와 `predict_log_proba()`메서드를 추가시킵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeWS7tiQ3t7r"
      },
      "source": [
        "### 5. 수백만 개의 샘플과 수백 개의 특성을 가진 훈련 세트에 SVM 모델을 훈련시키려면 원 문제와 쌍대 문제 중 어떤 것을 사용해야 하나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpOUt1GT3t5Q"
      },
      "source": [
        "- A. 커널 SVM은 쌍대 형식만 사용할 수 있기 때문에 이 질문은 선형 SVM에만 해당합니다. 원 문제의 계산 복잡도는 훈련 샘플 수 **m**에 비례하지만, 쌍대 형식의 계산 복잡도는 **m^2**과 **m^3**사이의 값에 비례합니다. 그러므로 수백만 개의 샘플이 있다면 쌍대 형식은 너무 느려질 것이므로 원 문제를 사용해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4QiISw53t2k"
      },
      "source": [
        "### 6. RBF 커널을 사용해 SVM 분류기를 훈련시켰더니 훈련 세트에 과소적합된 것 같습니다. **gamma**를 증가시켜야 할까요, 감소시켜야 할까요? **C**의 경우는 어떤가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkKNUIRl3t0J"
      },
      "source": [
        "- A. RBF 커널에 훈련된 SVM 분류기가 훈련 세트에 과소적합이라면 규제가 너무 큰 것일 수 있습니다. 규제를 줄이려면 **gamma**나 **C**(또는 둘 다) 값을 증가시켜야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA3kDTDY3txf"
      },
      "source": [
        "### 7. 이미 만들어진 QP 알고리즘 라이브러리를 사용해 소프트 마진 선형 SVM 분류기를 학습시키려면 QP 매개변수(**H**, **f**, **A**, **b**)를 어떻게 지정해야 하나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmwbRk223tug"
      },
      "source": [
        "- A. 하드 마진 문제에 대한 QP 파라미터를 **H'**, **f'**, **A'**,**b'**라고 하겠습니다(5.4.3절 '콰드라틱 프로그래밍' 참조). 소프트 마진 문제의 QP 파라미터는 **m**개의  추가적인 파라미터와 **m**개의 추가적인 제약을 가집니다. 이들을 다음과 같이 정의할 수 있습니다.\n",
        "    - **H**는 **H'**의 오른쪽에 0으로 채워진 m개의 열이 있고 아래에 0으로 채워진 m개의 열이 있는 행렬입니다.\n",
        "    - **f**는 **f'**에 하이퍼파라미터 C와 동일한 값의 원소 m개가 추가된 벡터입니다.\n",
        "    - **b**는 **b'**에 값이 0인 원소 m개가 추가된 벡터입니다.\n",
        "    - **A**는 **A'**의 오른 쪽에 m x n 단위 행렬 Im이 추가되고 바로 그 아래에 -Im이 추가되며 나머지는 0으로 채워진 행렬입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGoN2amB3thZ"
      },
      "source": [
        "연습문제 8, 9 10의 정답은 주피터 노트북을 참고하세요."
      ]
    }
  ]
}