{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 8. 차원 축소.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO4jaGRZcc6FHjE0YIZPGes"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9SVmm6LeIrH1"},"source":["## 8.7 연습문제"]},{"cell_type":"markdown","metadata":{"id":"wDOJ_KN7IrFy"},"source":["### 1. 데이터셋의 차원을 축소하는 주요 목적은 무엇인가요? 대표적인 단점은 무엇인가요?"]},{"cell_type":"markdown","metadata":{"id":"vB3ZYyxTIrD0"},"source":["- A. 차원 축소의 목적은 다음과 같습니다.\n","    - 훈련 알고리즘의 속도를 높이기 위해(어떤 경우에는 잡음과 중복된 특성을 삭제할 수도 있어 훈련 알고리즘의 성능을 높입니다.)\n","    - 데이터를 시각화하고 가장 중요한 특성에 대한 통찰을 얻기 위해\n","    - 메모리 공간을 절약하기 위해(압축)\n","\n","- 주요 단점은 다음과 같습니다.\n","    - 일부 정보를 잃어버려 훈련 알고리즘의 성능을 감소시킬 수 있습니다.\n","    - 계산 비용이 높습니다.\n","    - 머신러닝 파이프라인의 복잡도를 증가시킵니다.\n","    - 변환된 데이터를 이해하기 어려운 경우가 많습니다."]},{"cell_type":"markdown","metadata":{"id":"4kASd2IEIrB4"},"source":["### 2. 차원의 저주란 무엇인가요?"]},{"cell_type":"markdown","metadata":{"id":"2o_FWg5YIq_s"},"source":["- A. 차원의 저주는 저차원 공간에는 없는 많은 문제가 고차원 공간에서 일어난다는 사실을 뜻합니다. 머신러닝에서 무작위로 선택한 고차원 벡터는 매우 희소해서 과대적합의 위험성이 크고, 많은 양의 데이터가 있지 않으면 데이터에 있는 패턴을 잡아내기 매우 어려운 것이 흔한 현상입니다."]},{"cell_type":"markdown","metadata":{"id":"E49kE2y0Iq9M"},"source":["### 3. 데이터셋의 차원을 축소시키고 나서 이 작업을 원복할 수 있나요? 할 수 있다면 어떻게 가능할까요? 가능하지 않다면 왜일까요?"]},{"cell_type":"markdown","metadata":{"id":"z9VQRxzGIq6u"},"source":["- A. 여기에서 설명한 알고리즘 중 하나를 사용해 데이터셋의 차원이 축소되면 일부 정보가 차원 축소 과정에서 사라지기 때문에 이를 완벽하게 되돌리는 것은 불가능합니다. (PCA 같은) 일부 알고리즘은 비교적 원본과 비슷한 데이터셋을 재구성할 수 있는 간단한 역변환 방법을 가지고 있지만, (T-SNE 같은) 다른 알고리즘들은 그렇지 않습니다."]},{"cell_type":"markdown","metadata":{"id":"HHNgghnUIq4W"},"source":["### 4. 매우 비선형적인 데이터셋의 차원을 축소하는 데 PCA를 사용할 수 있을까요?"]},{"cell_type":"markdown","metadata":{"id":"D8R1tqxzIq1h"},"source":["- A. PCA는 불필요한 차원을 제거할 수 있기 때문에 매우 비선형적이더라도 대부분의 데이터셋에서 차원을 축소하는 데 사용할 수 있습니다. 그러나 불필요한 차원이 없다면(예를 들면 스위스 롤 데이터셋) PCA의 차원 축소는 너무 많은 정보를 잃게 만듭니다. 즉, 스위스 롤은 펼쳐야 하며 말려진 것을 뭉개면 안 됩니다."]},{"cell_type":"markdown","metadata":{"id":"DmNJBROPO0O7"},"source":["### 5. 설명된 분산을 95%로 지정한 PCA를 1,000개의 차원을 가진 데이터셋에 적용한다고 가정하겠습니다. 결과 데이터셋의 차원은 얼마나 될까요?"]},{"cell_type":"markdown","metadata":{"id":"mXD-W9ivO0MC"},"source":["- A. 이 질문에는 속임수가 있습니다. 답은 데이터셋에 따라 다릅니다. 극단적인 두 가지 사례를 살펴보겠습니다. 먼저 거의 완벽하게 일렬로 늘어선 데이터 포인트로 구성된 데이터셋을 생각해보겠습니다. 이 경우 PCA는 분산의 95%를 유지하면서 데이터셋을 단 하나의 차원으로 줄일 수 있습니다. 이번에는 완전히 무작위로 1,000개의 차원에 걸쳐 흩어져 있는 데이터셋을 생각해보겠습니다. 이 경우 분산의 95%를 보존하려면 거의 950개의 차원이 필요할 것입니다. 그러므로 답은 데이터셋에 따라 달라지고 1에서 950 사이의 어떤 수도 될 수 있습니다. 차원 수에 대한 함수로 설명된 분산의 그래프를 그려보는 것이 데이터셋에 내재된 차원 수를 대략 가늠할 수 있는 한 가지 방법입니다."]},{"cell_type":"markdown","metadata":{"id":"flgOzNUhO0Ju"},"source":["### 6. 기본 PCA, 점진적 PCA, 랜덤 PCA, 커널 PCA는 어느 경우에 사용될까요?"]},{"cell_type":"markdown","metadata":{"id":"rFQcMFG0O0HT"},"source":["- A. 기본 PCA가 우선적으로 사용되지만 데이터셋 크기가 메모리에 맞을 때에 가능합니다. 점진적 PCA는 메모리에 담을 수 없는 대용량 데이터셋에 적합합니다. 하지만 기본 PCA보다 느리므로 데이터셋이 메모리 크기에 맞으면 기본 PCA를 사용해야 합니다. 점진적 PCA는 새로운 샘플이 발생될 떄마다 실시간으로 PCA를 적용해야 하는 온라인 작업에 사용 가능합니다. 랜덤 PCA는 데이터셋이 메모리 크기에 맞고 차원을 크게 축소시킬 때 사용됩니다. 이 경우에는 기본 PCA보다 훨씬 빠릅니다. 커널 PCA는 비선형 데이터셋에 유용합니다."]},{"cell_type":"markdown","metadata":{"id":"PujCwQVMO0Eb"},"source":["### 7. 어떤 데이터셋에 적용한 차원 축소 알고리즘의 성능을 어떻게 평가할 수 있을까요?"]},{"cell_type":"markdown","metadata":{"id":"c-x5IH2jO0CA"},"source":["- A. 직관적으로 데이터셋에서 너무 많은 정보를 잃지 않고 차원을 많이 제거할 수 있다면 차원 축소 알고리즘이 잘 작동한 것입니다. 이를 측정하는 한 가지 방법은 역변환을 수행해서 재구성 오차를 측정하는 것입니다. 하지만 모든 차원 축소 알고리즘이 역변환을 제공하지는 않습니다. 만약 차원 축소를 다른 머신러닝 알고리즘(예를 들면 랜덤 포레스트 분류기)을 적용하기 전에 전처리 단계로 사용한다면 두 번째 알고리즘의 성능을 측정해볼 수 있습니다. 즉, 차원 축소가 너무 많은 정보를 잃지 않았다면 원본 데이터셋을 사용했을 때와 비슷한 성능이 나와야 합니다."]},{"cell_type":"markdown","metadata":{"id":"6MXklGbLQ8Nb"},"source":["### 8. 두 개의 차원 축소 알고리즘을 연결할 수 있을까요?"]},{"cell_type":"markdown","metadata":{"id":"RnMGKVypQ8KY"},"source":["- A. 당연히 두 개의 차원 축소 알고리즘을 연결할 수 있습니다. PCA로 불필요한 차원을 대폭 제거하고 난 다음 LLE 같이 훨씬 느린 알고리즘을 적용하는 것이 대표적인 사례입니다. 이런 2단계 방식은 LLE만 사용했을 때와 거의 비슷한 성능을 내지만 속도가 몇 분의 1로 줄어들 것입니다."]},{"cell_type":"markdown","metadata":{"id":"eD9MdwLVRB8C"},"source":["### 9. 생략"]},{"cell_type":"markdown","metadata":{"id":"owapeOdqRB2R"},"source":["### 10. 생략"]},{"cell_type":"code","metadata":{"id":"27GHarsMREX2"},"source":[""],"execution_count":null,"outputs":[]}]}