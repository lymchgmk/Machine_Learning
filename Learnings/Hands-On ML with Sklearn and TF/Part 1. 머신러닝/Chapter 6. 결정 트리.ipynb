{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 6. 결정 트리.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOJUEymf3G6LWvRMx97S2nA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7TApxYKpOSb6"},"source":["## 6.10 연습문제"]},{"cell_type":"markdown","metadata":{"id":"gBINFQ0pOUHw"},"source":["### 1. 백만 개의 샘플을 가진 훈련 세트에서 (규제 없이) 훈련시킨 결정 트리의 깊이는 대략 얼마일까요?"]},{"cell_type":"markdown","metadata":{"id":"FiAfhDt7OUBU"},"source":["- A.m개의 리프 노드를 포함한 균형이 잘 잡힌 이진 트리의 깊이는 log2(m)을 반올림한 것과 같습니다. 이진 결정 트리(사이킷런에 있는 모든 트리는 가지가 두 개입니다)를 제한을 둦 않고 훈련시키면 훈련 샘플마다 하나의 리프 노드가 되므로 어느 정도 균형이 잘 잡힌 트리가 됩니다. 따라서 훈련 세트에 백만 개 샘플이 있다면 결정 트리의 깊이는 log2(10^6) = 20이 될 것입니다(실제로는 완벽하게 균형 잡힌 트리가 만들어지지 않기 때문에 조금 더 늘어납니다)."]},{"cell_type":"markdown","metadata":{"id":"DJYD39--OT6D"},"source":["### 2. 한 노드의 지니 불순도가 보통 그 부모 노드보다 작을까요, 아니면 클까요? 일반적으로 작거나 클까요, 아니면 항상 작거나 클까요?"]},{"cell_type":"markdown","metadata":{"id":"eY_PIvvqOTyN"},"source":["- A. 한 노드의 지니 불순도는 일반적으로 부모의 불순도보다 낮습니다. 이는 자식의 지니 불순도의 합이 최소화되는 방향으로 각 노드를 분할하는 CART 훈련 알고리즘의 비용 함수 때문입니다. 그러나 다른 자식 노드의 지니 불순도 감소량이 어떤 노드의 불순도 증가량보다 큰 경우라면 부모의 불순도보다 큰 노드가 생길 수 있습니다. 예를 들어 클래스 A의 샘플을 4개, 클래스 B의 샘플을 1개 포함한 노드를 생각해보겠습니다. 이 노드의 지니 불순도는 0.32입니다. 이 데이터셋은 1차원이고 A, B, A, A, A 순으로 늘어서 있다고 가정하겠습니다. 알고리즘이 이 노드를 두 번쨰 샘플 이후에 나누어 샘플 A, B를 가진 자식 노드와 샘플 A, A, A를 가진 자식 노드를 만듭니다. 첫 번째 자식 노드의 지니 불순도는 0.5가 되어 부모보다 큽니다. 이는 다른 노드가 순수 노드가 되는 것에 대한 대가입니다. 가중치를 준 전체 지니 불순도는 0.2가 되어 부모의 지니 불순도보다 낮습니다."]},{"cell_type":"markdown","metadata":{"id":"SxxV_FjaOTcl"},"source":["### 3. 결정 트리가 훈련 세트에 과대적합되었다면 `max_depth`를 줄이는 것이 좋을까요?"]},{"cell_type":"markdown","metadata":{"id":"SHKF8w5aQd29"},"source":["- A. 결정 트리가 훈련 세트에 과대적합되었다면 모델에 제약을 가해 규제해야 하므로 `max_depth`를 낮추는 것이 좋습니다."]},{"cell_type":"markdown","metadata":{"id":"_InNcGpBQd0_"},"source":["### 4. 결정 트리가 훈련 세트에 과소적합되었다면 입력 특성의 스케일을 조정하는 것이 좋을까요?"]},{"cell_type":"markdown","metadata":{"id":"Nl-AwkYaQdyU"},"source":["- A. 결정 트리는 훈련 데이터의 스케일이나 원점에 맞추어져 있는지 상관하지 않습니다. 이것이 결정 트리의 장점 중 하나입니다. 그러므로 결정 트리가 훈련 세트에 과소적합되었다고 입력 특성의 스케일을 조정하는 것은 시간 낭비입니다."]},{"cell_type":"markdown","metadata":{"id":"zg2Qc7QjQdvm"},"source":["### 5. 백만 개의 샘플을 가진 훈련 세트에 결정 트리를 훈련시키는 데 한 시간이 걸렸다면, 천만 개의 샘플을 가진 훈련 세트에 결정 트리를 훈련시키는 데는 대략 얼마나 걸릴까요?"]},{"cell_type":"markdown","metadata":{"id":"r1byjq8VQdsz"},"source":["- A. 결정 트리 훈련의 계산 복잡도는 O(nmlog(m))입니다. 그러므로 훈련 세트의 크기에 10을 곱하면 훈련 시간은 K=10log(10m)/log(m)배 늘어납니다. 만약 m=10^6이면 K=11.7이므로 훈련에 대략 11.7시간이 걸릴 것으로 예상할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"s39IX9hyQdp4"},"source":["### 6. 십만 개의 샘플을 가진 훈련 세트가 있다면 `presort=True`로 지정하는 것이 훈련 속도를 높일까요?"]},{"cell_type":"markdown","metadata":{"id":"0AEL10uRpfTU"},"source":["- A. 데이터셋의 샘플 수가 수천 개 미만일 때 훈련 세트를 사전에 정렬하여 훈련 속도를 높일 수 있습니다. 100,000개의 샘플을 포함하고 있을 때 `presort=True`로 지정하면 훈련 속도가 매우 느려질 것입니다."]},{"cell_type":"markdown","metadata":{"id":"DagJM5mIpf2C"},"source":["### 7. 다음 단계를 따라 moons 데이터셋에 결정 트리를 훈련시키고 세밀하게 튜닝해보세요."]},{"cell_type":"markdown","metadata":{"id":"8-gloH2ppgBD"},"source":["### 8. 다음 단계를 따라 랜덤 포레스트를 만들어보세요."]},{"cell_type":"code","metadata":{"id":"LM86jysrp99U"},"source":[""],"execution_count":null,"outputs":[]}]}