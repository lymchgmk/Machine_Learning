{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 11. 심층 신경망 훈련하기.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMbn5zhLUAEHWhIwVVuRP31"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gl6eAKzgUZsO"},"source":["## 11.6 연습문제"]},{"cell_type":"markdown","metadata":{"id":"Zrx-36p2UZu_"},"source":["### 1. He 초기화를 사용해서 무작위로 선택한 값이라면 모든 가중치를 같은 값으로 초기화해도 괜찮을까요?"]},{"cell_type":"markdown","metadata":{"id":"rNcHKklsUZxN"},"source":["- A. 아니오. 모든 가중치는 독립적으로 샘플링되어야 합니다. 즉, 같은 초깃값을 가지면 안 됩니다. 가중치를 무작위로 샘플링하는 중요한 한 가지 목적은 대칭성을 피하기 위함입니다. 모든 가중치가 0이 아니더라도 같은 초깃값을 가지면 대칭성이 깨지지 않고(즉, 어떤 층에 있는 모든 뉴런이 동일합니다) 역전파도 이를 해결할 수 없을 것입니다. 구체적으로 말하면 어떤 층에 있는 모든 뉴런이 항상 같은 가중치를 가지게 됩니다. 이는 층마다 하나의 뉴런이 있는 것과 같으므로 수렴하는 데 오랜 시간이 걸립니다. 이런 구성으로 좋은 솔루션으로 수렴하는 것은 사실상 불가능합니다."]},{"cell_type":"markdown","metadata":{"id":"oJH_fWGrUZz1"},"source":["### 2. 편향을 0으로 초기화해도 괜찮을까요?"]},{"cell_type":"markdown","metadata":{"id":"qnYrVlttUZ2S"},"source":["- A. 편향을 0으로 초기화 하는 것은 아무 상관이 없습니다. 또는 편향을 가중치처럼 초기화해도 괜찮습니다. 이들은 큰 차이를 만들지 않습니다."]},{"cell_type":"markdown","metadata":{"id":"_XMBX_RvUZ43"},"source":["### 3. ReLU보다 SELU 활성화 함수가 나은 세 가지는 무엇인가요?"]},{"cell_type":"markdown","metadata":{"id":"qDtl93V6UZ7c"},"source":["- A. \n","    - 이 함수는 음수를 받을 수 있어서 뉴런의 평균 출력이 (음수는 절대 출력하지 않는) ReLU 활성화 함수보다 일반적으로 0에 더 가깝습니다. 이는 그래디언트 소실 문제를 완화시켜줍니다.\n","    - 도함수의 값은 항상 0이 아니라서 ReLU 유닛에서 일어나는 죽은 뉴런 현상을 피할 수 있습니다.\n","    - (순차 모델이고, 르쿤 초기화를 사용해 가중치를 초기화하고, 입력을 표준화하고, 드롭아웃이나 L1 규제같이 SELU와 호환되지 않는 층이나 규제가 없다는) 조건이 맞을 때 SELU 활성화 함수를 사용하면 모델이 자기 정규화되어 그레이디언트 폭주나 소실 문제를 해결하는 데 도움이 됩니다."]},{"cell_type":"markdown","metadata":{"id":"N_XPE5f8UZ9l"},"source":["### 4. 어떤 경우에 SELU, LeakyReLU(또는 그 변종), ReLU, tanh, 로지스틱, 소프트맥스와 같은 활성화 함수를 사용해야 하나요?"]},{"cell_type":"markdown","metadata":{"id":"iiym9O3MUZ_4"},"source":["- A. SELU 함수가 기본값으로 좋습니다. 가능한 한 빠른 신경망을 원한다면 대신 LeakyReLU의 변종을 사용할 수 있습니다(예를 들면 기본 하이퍼파라미터를 사용한 LeakyReLU). 일반적으로 SELU와 LeakyReLU의 성능이 더 뛰어남에도 ReLU 활성화 함수가 간단하기 때문에 많은 사람이 선호합니다. 그러나 어떤 경우에는 정확히 0을 출력하는 ReLU 활성화 함수의 기능이 유용할 수 있습니다(17장 참조). 또한 이따금 하드웨어 가속은 물론 최적화된 구현의 덕을 입을 수 있습니다. 하지만 최근에는 (순환 신경망을 제외한) 은닉층을 많이 사용하진 않습니다. 로지스틱 활성화 함수는 확률을 추정할 필요가 있을 때도 사용합니다(예를 들어 이진 분류). 하지만 은닉층에는 거의 사용되지 않습니다(예외가 있습니다. 예를 들어 17장에서 변이형 오토인코더의 코딩 층에 사용됩니다). 마지막으로 소프트맥스 활성화 함수는 상호 배타적인 클래스에 대한 확률을 출력하는 출력층에 사용됩니다. 하지만 그 외에 은닉층에는 거의 사용되지 않습니다."]},{"cell_type":"markdown","metadata":{"id":"7NryGVm7UaCE"},"source":["### 5. SGD 옵티마이저를 사용할 때 momentum 하이퍼파라미터를 너무 1에 가깝게 하면(예를 들면 0.99999) 어떤 일이 일어날까요?"]},{"cell_type":"markdown","metadata":{"id":"usFh9zwfUaEL"},"source":["- A. `momentum` 하이퍼파라미터를 1에 가깝게 셋팅하면 알고리즘이 전역 최적점 방향으로 빠르게 진행되겠지만 모멘텀 때문에 최솟값을 지나치게 될 것입니다. 그런 다음 느려져서 되돌아오고, 다시 가속되어 또 지나치게 되는 식입니다. 수렴하기 전에 여러 번 이렇게 진동을 하게 됩니다. 그러므로 작을 `momentum`값을 사용했을 때보다 전반적으로 수렴하는 데 훨씬 오래 걸릴 것입니다."]},{"cell_type":"markdown","metadata":{"id":"2-dxNGHBUaGV"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"J7V8p2nBUaJO"},"source":["### 6. 희소 모델을 만들 수 있는 세 가지 방법은 무엇인가요?"]},{"cell_type":"markdown","metadata":{"id":"7gQteq-UUaLr"},"source":["- A. 희소 모델 (즉, 대부분의 가중치가 0인 모델)을 만드는 첫 번째 방법은 평범하게 모델을 훈련시키고 작은 가중치를 0으로 만드는 것입니다. 두 번째 방법은 (더욱 희소하게 만들려면) 훈련하는동안 옵티마이저에 희소한 모델을 만들도록 L1 규제를 사용하는 것입니다. 세 번째 방법은 텐서플로의 모델 최적화 도구를 사용하는 것입니다."]},{"cell_type":"markdown","metadata":{"id":"6azCpe8RUaOY"},"source":["### 7. 드롭아웃이 훈련 속도를 느리게 만드나요? 추론(Inference, 새로운 샘플에 대한 예측을 만드는 것)도 느리게 만드나요? MC 드롭아웃은 어떤가요?"]},{"cell_type":"markdown","metadata":{"id":"NnVkM3afUaRE"},"source":["- A. 예. 드롭아웃은 일반적으로 대략 두 배 정도 훈련 속도를 느리게 만듭니다. 그러나 드롭아웃은 훈련할 때만 적용되므로 추론 속도에는 영향을 미치지 않습니다. MC 드롭아웃은 훈련하는 동안에는 드롭아웃과 똑같습니다. 하지만 추론할 떄도 작동하기 때문에 각 추론의 속도가 조금씩 느려집니다. 더 중요한 것은 MC 드롭아웃을 사용할 때 더 나은 예측을 얻기 위해서 10배 이상 추론을 많이 실행한다는 사실입니다. 이 말은 10배 이상 예측을 만드는 속도가 느려진다는 뜻입니다."]},{"cell_type":"markdown","metadata":{"id":"fDAqErZPvMd6"},"source":["### 8. 생략"]}]}