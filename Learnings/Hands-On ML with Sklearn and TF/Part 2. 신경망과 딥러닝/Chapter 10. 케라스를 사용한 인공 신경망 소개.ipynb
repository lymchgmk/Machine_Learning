{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 10. 케라스를 사용한 인공 신경망 소개.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOFlrsrSa6OSe5tgou2vyBB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vOLip88oAGX7"},"source":["## 10.4 연습문제"]},{"cell_type":"markdown","metadata":{"id":"ZANg9aycAGVj"},"source":["### 1. 생략"]},{"cell_type":"markdown","metadata":{"id":"7CRFyeIeAGSW"},"source":["### 2. 생략"]},{"cell_type":"markdown","metadata":{"id":"RUKgdgmVAGP5"},"source":["### 3. 고전적인 퍼셉트론(즉, 퍼셉트론 훈련 알고리즘으로 훈련된 단일 TLU)보다 로지스틱 회귀 분류기를 일반적으로 선호하는 이유는 무엇인가요? 퍼셉트론을 어떻게 수정하면 로지스틱 회귀 분류기와 동등하게 만들 수 있나요?"]},{"cell_type":"markdown","metadata":{"id":"ElzruLNQGKph"},"source":["- A. 고전적인 퍼셉트론은 데이터셋이 선형적으로 구분될 때만 수렴하고 클래스 확률을 추정할 수 없습니다. 이와는 반대로 로지스틱 회귀 분류기는 데이터셋이 선형적으로 구분되지 못해도 좋은 솔루션으로 수렴하고 클래스확률을 출력합니다. 퍼셉트론의 활성화 함수를 로지스틱 활성화 함수로(또는 여러 개의 뉴런일 경우 소프트맥스 활성화 함수로) 바꾸고, 경사 하강법을 사용하여(또는 크로스 엔트로피 같은 비용 함수를 최소화하는 다른 최적화 알고리즘을 사용하여) 훈련시키면 로지스틱 회귀 분류기와 동일하게 됩니다."]},{"cell_type":"markdown","metadata":{"id":"gCaXE2U1AGNK"},"source":["### 4. 왜 초창기의 다층 퍼셉트론을 훈련할 때 로지스틱 활성화 함수가 핵심 요소였나요?"]},{"cell_type":"markdown","metadata":{"id":"wOtIc0dbGK6P"},"source":["- A. 로지스틱 확성화 함수의 도함수는 어디에서나 0이 아니어서 경사 하강법이 항상 경사를 따라 이동할 수 있으므로 초창기 MLP의 핵심 요소였습니다. 활성화 함수가 계단 함수일 때는 경사가 없기 때문에 경사 하강법이 이동할 수 없습니다."]},{"cell_type":"markdown","metadata":{"id":"Z3JV3wFyGxVW"},"source":["### 5. 인기 많은 활성화 함수 세 가지는 무엇인가요? 이를 그려볼 수 있나요?"]},{"cell_type":"markdown","metadata":{"id":"k8_gRgxCGxSy"},"source":["- A. 인기 있는 활성화 함수는 계단 함수, 로지스틱 함수(시그모이드), 하이퍼볼릭 탄젠트 함수, ReLU 함수 등입니다. ELU나 ReLU의 다른 변종은 11장을 참조하세요."]},{"cell_type":"markdown","metadata":{"id":"Vu9hgFjxGxQR"},"source":["### 6. 통과 뉴런 10개로 구성된 입력층, 뉴런 50개로 구성된 은닉층, 뉴런 3개로 구성된 출력층으로 이루어진 다층 퍼셉트론이 있다고 가정합시다. 모든 뉴런은 ReLU 활성화 함수를 사용합니다."]},{"cell_type":"markdown","metadata":{"id":"2cH3nsyLGxNt"},"source":["- 입력 행렬 X의 크기는 얼마인가요?\n","    - A. 입력 행렬 X의 크기는 m x 10입니다. m은 훈련 배치의 크기를 나타냅니다.\n","\n","- 은닉층의 가중치 벡터 Wb와 편향 벡터 bh의 크기는 얼마인가요?\n","    - A. 10 x 50, 50\n","\n","- 출력층의 가중치 벡터 Wa와 편향 벡터 bo의 크기는 얼마인가요?\n","    - A. 50 x 3, 3\n","\n","- 네트워크의 출력 행렬 Y의 크기는 얼마인가요?\n","    - A. m x 3\n","\n","- X, Wb, bh, Wa, bo의 함수로 네트워크의 출력 행렬 Y를 계산하는 식을 써보세요.\n","    - A. Y=ReLU(ReLU(XWb + bh)Wo + bo), ReLU 함수는 행렬에 있는 음수를 무조건 0으로 만듭니다. 편향 벡터를 행렬에 더하면 행렬의 모든 행에 덧셈이 각기 적용되는 브로드캐스팅이 일어납니다."]},{"cell_type":"markdown","metadata":{"id":"ceC5z9zaGxLG"},"source":["### 7. 스팸 메일을 분류하기 위해서는 출력층에 몇 개의 뉴런이 필요할까요? 출력층에 어떤 활성화 함수를 사용해야 할까요? MNIST 문제라면 출력층에 어떤 활성화 함수를 사용하고 뉴런은 몇 개가 필요할까요? 2장에서 본 주택 가격 예측용 네트워크에 대해 같은 질문의 답을 찾아보세요."]},{"cell_type":"markdown","metadata":{"id":"limZ04ZSGxIo"},"source":["- A. 스팸 메일을 분류하기 위해서는 신경망의 출력층에 하나의 뉴런만 필요합니다. 예를 들어 이메일이 스팸일 확률을 출력합니다. 확률을 추정할 때 일반적으로 출력층에 로지스틱 활성화 함수를 사용합니다. MNIST 문제라면 출력층에 10개의 뉴런이 필요하고, 다중 클래스 환경에서 클래스마다 하나의 확률을 출력하기 위해 로지스틱 함수를 소프트맥스 활성화 함수로 바꾸어야 합니다. 2장에서처럼 주택 가격을 예측하는 신경망을 만들고 싶다면 출력층에 활성화 함수가 없는 출력 뉴런 하나가 필요합니다."]},{"cell_type":"markdown","metadata":{"id":"flY_AEJdGxF1"},"source":["### 8. 역전파란 무엇이고 어떻게 작동하나요? 역전파와 후진 모드 자동 미분의 차이점은 무엇인가요?"]},{"cell_type":"markdown","metadata":{"id":"PwKoa6la4s4F"},"source":["- A. 역전파는 인공 신경망을 훈련시키는 하나의 기법입니다. 먼저 모델의 모든 파라미터(모든 가중치와 편향)에 대한 비용 함수의 그래디언트를 계산하고, 이 그래디언트를 사용해 경사 하강법 스텝을 수행합니다. 역전파 단계는 모델 파라미터가 비용 함수를 (희망하건데)최소화하는 값으로 수렴할 때까지 훈련 배치에서 일반적으로 수천 혹은 수백만 번 수행됩니다. 그래디언트를 계산하기 위해 역전파는 후진 모드 자동 미분을 사용합니다(역전파가 발명되었을 때는 이렇게 불리진 않았지만 이 기술은 역사적으로 여러 번 재발명되었습니다). 후진 모드 자동 미분은 계산 그래프의 정방향 계산에서 현재 훈련 배치에 대한 모든 노드의 값을 구합니다. 그 다음에 역방향 계산에서 한번에 모든 그래디언트를 구합니다(자세한 내용은 부록 D를 참조하세요). 그렇다면 무엇이 다른 걸까요? 역전파는 그래디언트 계산과 경사 하강법 스텝을 여러 번 수행하여 인공 신경망을 훈련시키는 전체 프로세스를 의미합니다. 이와 다르게 후진 모드 자동 미분은 그래디언트를 효과적으로 계산하는 하나의 기법으로 역전파에서 사용됩니다."]},{"cell_type":"markdown","metadata":{"id":"0PTGMCBn4s1q"},"source":["### 9. 다층 퍼셉트론에서 조정할 수 있는 하이퍼파라미터를 모두 나열해보세요. 훈련 데이터에 다층 퍼셉트론이 과대적합되었다면 이를 해결하기 위해 하이퍼파라미터를 어떻게 조정해야 할까요?"]},{"cell_type":"markdown","metadata":{"id":"90GmpUYq4syk"},"source":["- A. 기본 MLP에서 바꿀 수 있는 하이퍼파라미터는 은닉층 수, 각 은닉층의 뉴런 수, 각 은닉층과 출력층에서 사용하는 활성화 함수입니다. 일반적으로 ReLU(또는 이 함수의 변종, 11장 참조)가 은닉층의 활성화 함수 기본값으로 좋습니다. 출력층에서는 일밙거으로 이진분류에서는 로지스틱 활성화 함수, 다중 분류에서는 소프트맥스 활성화 함수를 사용하고 회귀에서는 활성화 함수를 적용하지 않습니다. MLP가 과대적합되었다면 은닉층 수와 각 은닉츠엥 있는 뉴런 수를 줄여볼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"lZj-eMF_4sut"},"source":["### 10. 생략"]}]}