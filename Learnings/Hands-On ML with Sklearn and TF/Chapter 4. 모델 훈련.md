# Chapter 4. 모델 훈련

## 4.7 연습문제

1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?
   - A. 수맥만 개의 특성이 있는 훈련 세트를 가지고 있다면 확률적 경사 하강법(SGD)이나 미니배치 경사 하강법을 사용할 수 있습니다. 훈련 세트가 메모리 크기에 맞으면 배치 경사 하강법도 가능합니다. 하지만 정규방정식이나 SVD 방법은 계산 복잡도가 특성 개수에 따라 매우 빠르게 증가하기 때문에 사용할 수 없습니다.
2. 훈련 세트에 있는 특성들이 각기 아주 다른 스케일을 가지고 있습니다. 이런 데이터에 잘 작동하지 않는 알고리즘은 무엇일까요? 그 이유는 무엇일까요? 이 문제를 어떻게 해결할 수 있을까요?
   - A. 훈련 세트에 있는 특성의 스케일이 매우 다르면 비용 함수는 길쭉한 타원 모양의 그릇 형태가 됩니다. 그래서 경사 하강법(GD) 알고리즘이 수렴하는 데 오랜 시간이 걸릴 것입니다. 이를 해결하기 위해서는 모델을 훈련하기 전에 데이터의 스케일을 조절해야 합니다. 정규방정식이나 SVD 방법은 스케일 조정 없이도 잘 작동합니다. 또한 규제가 있는 모델은 특성의 스케일이 다르면 지역 최적점에 수렴할 가능성이 있습니다. 규제는 가중치가 커지지 못하게 제약을 가하므로 특성값이 작으면 큰 값을 가진 특성에 비해 무시되는 경향이 있습니다.
3. 경사 하강법으로 로지스틱 회귀 모델을 훈련시킬 때 지역 최솟값에 갇힐 가능성이 있을까요?
   - A. 로지스틱 회귀 모델의 비용 함수는 볼록 함수이므로 경사 하강법이 훈련될 때 지역 최솟값에 갇힐 가능성이 없습니다.

4. 충분히 오랫동안 실행하면 모든 경사 하강법 알고리즘이 같은 모델을 만들어낼까요?
   - A. 최적화할 함수가 (선형 회귀나 로지스틱 회귀처럼) 볼록 함수이고 학습률이 너무 크지 않다고 가정하면 모든 경사 하강법 알고리즘이 전역 최적값에 도달하고 결국 비슷한 모델을 만들 것입니다. 하지만 학습률을 점진적으로 감소시키지 않으면 SGD와 미니배치 GD는 진정한 최적점에 수렴하지 못할 것입니다. 대신 전역 최적점 주변을 이리저리 맴돌게 됩니다. 이 말은 매우 오랫동안 훈련을 해도 경사 하강법 알고리즘들은 조금씩 다른 모델을 만들게 된다는 뜻입니다.
5. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?
   - A. 에포크마다 검증 에러가 지속적으로 상승한다면 한 가지 가능성은 학습률이 너무 높고 알고리즘이 발산하는 것일지 모릅니다. 훈련 에러도 올라간다면 이 문제가 확실하고 학습률을 낮추어야 합니다. 그러나 훈련 에러가 올라가지 않는다면 모델이 훈련 세트에 과대적합되어 있는 것이므로 훈련을 멈추어야 합니다.
6. 검증 오차가 상승하면 미니배치 경사 하강법을 즉시 중단하는 것이 좋은 방법인가요?
   - A. 무작위성 떄문에 확률적 경사 하강법이나 미니배치 경사 하강법 모두 매 훈련 반복마다 학습의 진전을 보장하지 못합니다. 검증 에러가 상승될 때 훈련을 즉시 멈춘다면 최적점에 도달하기 전에 너무 일찍 멈추게 될지 모릅니다. 그 다음 더 나은 방법은 정기적으로 모델을 저장하고 오랫동안 진전이 없을 때(즉, 최상의 점수를 넘어서지 못하면). 저장된 것 중 가장 좋은 모델로 복원하는 것입니다.
7. (우리가 언급한 것 중에서) 어떤 경사 하강법 알고리즘이 가장 빠르게 최적 솔루션의 주변에 도달할까요? 실제로 수렴하는 것은 어떤 것인가요? 다른 방법들도 수렴하게 만들 수 있나요?
   - A. 확률적 경사 하강법은 한 번에 하나의 훈련 샘플만 사용하기 떄문에 훈련 반복이 가장 빠릅니다. 그래서 가장 먼저 전역 최적점 근처에 도달합니다(그다음이 작은 미니배치 크기를 가진 미니배치 GD입니다). 그러나 훈련 시간이 충분하면 배치 경사 하강법만 실제로 수렴할 것입니다. 앞서 언급한 대로 학습률을 점진적으로 감소시키지 않으면 SGD와 미니배치 GD는 최적점 주변을 맴돌 것입니다.

8. 다항 회귀를 사용했을 때 학습 곡선을 보니 훈련 오차와 검증 오차 사이에 간격이 큽니다. 무슨 일이 생긴 걸까요? 이 문제르 해결하는 세 가지 방법은 무엇인가요?
   - A. 검증 오차가 훈련 오차보다 훨씬 높으면 모델이 훈련 세트에 과대적합되었기 때문일 가능성이 높습니다. 이를 해결하는 첫 번째 방법은 다항 차수를 낮추는 것입니다. 자유도를 줄이면 과대적합이 훨씬 줄어들 것입니다. 두 번째 방법은 모델을 규제하는 것입니다. 예를 들어 비용함수에 L2 페널티(릿지)나 L1 페널티(라쏘)를 추가합니다. 이 방법도 모델의 자유도를 감소시킵니다. 세 번째 방법은 훈련 세트의 크기를 증가시키는 것입니다.

9. 릿지 회귀를 사용했을 때 훈련 오차와 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에서는 높은 변항이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 a를 증가시켜야 할까요, 아니면 줄여야 할까요?

   - A. 훈련 에러와 검증 에러가 거의 비슷하고 매우 높다면 모델이 훈련 세트에 과소적합되었을 가능성이 높습니다. 즉, 높은 편향을 가진 모델입니다. 따라서 규제 하이퍼파라미터 a를 감소시켜야 합니다.

10. 다음과 같이 사용해야 하는 이유는?

    - 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀
      - A. 규제가 있는 모델이 일반적으로 규제가 없는 모델보다 성능이 좋습니다. 그래서 평범한 선형 회귀보다 릿지 회귀가 선호됩니다.
    - 릿지 회귀 대신 라쏘 회귀
      - A. 라쏘 회귀는 L1 페널티사용하여 가중치를완전히 0으로 만드는 경향이 있습니다. 이는 가장 중요한 가중치를 제외하고는 모두 0이 되는 희소한 모델을 만듭니다. 또한 자동으로 특성 선택의 효과를 가지므로 단지 몇 개의 특성만 실제 유용할 것이라고 의심될 때 사용하면 좋습니다. 만약 확신이 없다면 릿지 회귀를 사용해야 합니다.

    - 라쏘 회귀 대신 엘라스틱넷
      - A. 라쏘가 어떤 경우(몇 개의 특성이 강하게 연관되어 있거나 훈련 샘플보다 특성이 더 많을 때)에는 불규칙하게 행동하므로 엘라스틱넷이 라쏘보다 일반적으로 선호됩니다. 그러나 추가적인 하이퍼파라미터가 생깁니다. 불규칙한 행동이 없는 라쏘를 원하면 엘라스틱 넷에 L1_ratio를 1에 가깝게 설정하면 됩니다.

11. 사진을 낮과 밤, 실내와 실외로 분류하려 합니다. 두 개의 로지스틱 회귀 분류기를 만들어야 할까요, 아니면 하나의 소프트맥스 회귀 분류기를 만들어야 할까요?

    - A. 실외와 실내, 낮과 밤에 따라 사진을 구분하고 싶다면 이 둘은 배타적인 클래스가 아니기 때문에(즉, 네 가지 조합이 모두 가능하므로) 두 개의 로지스틱 회귀 분류기를 훈련시켜야 합니다.

12. 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요).